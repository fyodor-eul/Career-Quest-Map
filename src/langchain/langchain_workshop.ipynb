{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_header",
   "metadata": {},
   "source": [
    "# LangChain Workshop: Building an AI-Powered Chatbot\n",
    "### CCDS Tech for Good 2026 Hackathon\n",
    "\n",
    "**Workshop Goals:**\n",
    "- Understand LangChain fundamentals\n",
    "- Build conversational AI agents\n",
    "- Create production-ready chatbot classes\n",
    "- Master prompt engineering techniques\n",
    "\n",
    "**What you'll build:**\n",
    "- Basic chatbot with memory\n",
    "- AI companion with personality\n",
    "- Tool-using agents\n",
    "- Reusable chatbot framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basics_header",
   "metadata": {},
   "source": [
    "# 1. LangChain Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first_call_header",
   "metadata": {},
   "source": [
    "## 1.1 Your First LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "first_call_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for building AI applications with large language models,提供 modular components like prompts, chains, and agents, plus easy integrations with tools and data sources.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-5-nano\",\n",
    ")\n",
    "\n",
    "# Create messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Explain what LangChain is in one sentence.\")\n",
    "]\n",
    "\n",
    "# Get response\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding_components",
   "metadata": {},
   "source": [
    "###  Understanding the Components\n",
    "\n",
    "**1. SystemMessage**: Defines the AI's role and behavior\n",
    "- Sets personality and expertise\n",
    "- Provides context and constraints\n",
    "- Remains consistent across conversation\n",
    "\n",
    "**2. HumanMessage**: User's input to the AI\n",
    "- The query or prompt\n",
    "- Changes with each interaction\n",
    "\n",
    "**3. Temperature**: Controls randomness (0-1)\n",
    "- `0.0` → Deterministic, focused\n",
    "- `0.7` → Balanced (recommended)\n",
    "- `1.0` → Creative, varied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_header",
   "metadata": {},
   "source": [
    "## 2.2 Adding Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "memory_basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1:\n",
      "AI: Hi Sarah! Nice to meet you. How can I help today? I can answer questions, help with writing, brainstorm ideas, plan projects, practice a skill, or just chat. Tell me a bit about what you’re interested in or what you’d like to work on, and we’ll take it from there.\n",
      "\n",
      "Turn 2:\n",
      "AI: Your name is Sarah. Nice to meet you again, Sarah—would you like me to call you by a nickname or just Sarah?\n",
      "\n",
      "Turn 3:\n",
      "AI: We talked about your name. You said you’re Sarah, I confirmed that and asked whether you’d like me to call you Sarah or use a nickname. Would you prefer a nickname, or should I just stick with Sarah?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# 1. Initialize the Azure LLM\n",
    "# Ensure 'langchain-openai' is installed via: uv add langchain-openai\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-5-nano\"\n",
    ")\n",
    "\n",
    "# 2. Setup the Prompt with a 'history' placeholder\n",
    "# This replaces the internal 'ConversationChain' template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 3. Create the Chain (LCEL pipe syntax)\n",
    "chain = prompt | llm\n",
    "\n",
    "# 4. Define Memory Storage\n",
    "# Using ChatMessageHistory instead of the legacy ConversationBufferMemory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 5. Wrap the chain with History logic\n",
    "# This is the modern replacement for ConversationChain\n",
    "conversation_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# 6. Execution\n",
    "config = {\"configurable\": {\"session_id\": \"robot_dev_test\"}}\n",
    "\n",
    "print(\"Turn 1:\")\n",
    "res1 = conversation_with_history.invoke({\"input\": \"Hi! My name is Sarah.\"}, config)\n",
    "print(f\"AI: {res1.content}\\n\")\n",
    "\n",
    "print(\"Turn 2:\")\n",
    "res2 = conversation_with_history.invoke({\"input\": \"What's my name?\"}, config)\n",
    "print(f\"AI: {res2.content}\\n\")\n",
    "\n",
    "print(\"Turn 3:\")\n",
    "res3 = conversation_with_history.invoke({\"input\": \"What did we just talk about?\"}, config)\n",
    "print(f\"AI: {res3.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_types_header",
   "metadata": {},
   "source": [
    "## 2.3 Different Memory Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "memory_types_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Buffer Memory ===\n",
      "[HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello!', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Window Memory (k=2) ===\n",
      "HumanMessage: Message 2\n",
      "AIMessage: Response 2\n",
      "HumanMessage: Message 3\n",
      "AIMessage: Response 3\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Summary Memory ===\n",
      "Updated Summary: Updated summary: We started with a general ML question and then discussed applying machine learning to Underwater Unmanned Vehicles (UUVs), including how ML can aid navigation, perception, control, autonomy, and mission planning in underwater environments, along with challenges (limited bandwidth, acoustic comms, sensing noise) and approaches (supervised/unsupervised learning, reinforcement learning, onboard edge processing).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# Initialize LLM for Summary Memory\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-5-nano\"\n",
    ")\n",
    "\n",
    "# --- 1. Buffer Memory (Stores Everything) ---\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hi!\")\n",
    "history.add_ai_message(\"Hello!\")\n",
    "print(\"=== Buffer Memory ===\")\n",
    "print(history.messages)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 2. Window Memory (Last K interactions) ---\n",
    "# In modern LangChain, you simply slice the message list.\n",
    "# This is much more 'Pythonic' and efficient for high-performance C++ integration.\n",
    "all_messages = [\n",
    "    HumanMessage(content=\"Message 1\"), AIMessage(content=\"Response 1\"),\n",
    "    HumanMessage(content=\"Message 2\"), AIMessage(content=\"Response 2\"),\n",
    "    HumanMessage(content=\"Message 3\"), AIMessage(content=\"Response 3\"),\n",
    "]\n",
    "k = 2\n",
    "window_messages = all_messages[-(k*2):] # Multiply by 2 because 1 interaction = User + AI\n",
    "print(f\"=== Window Memory (k={k}) ===\")\n",
    "for msg in window_messages:\n",
    "    print(f\"{type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 3. Summary Memory (LLM-based Summarization) ---\n",
    "# Instead of a hidden class, you explicitly ask the LLM to summarize.\n",
    "def summarize_messages(existing_summary, new_messages):\n",
    "    prompt = f\"Current summary: {existing_summary}\\n\\nNew messages: {new_messages}\\n\\nUpdate the summary.\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "current_summary = \"The user is asking about ML.\"\n",
    "new_talk = \"We discussed how it applies to Underwater Unmanned Vehicles (UUVs).\"\n",
    "updated_summary = summarize_messages(current_summary, new_talk)\n",
    "\n",
    "print(\"=== Summary Memory ===\")\n",
    "print(f\"Updated Summary: {updated_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompts_header",
   "metadata": {},
   "source": [
    "## 2.4 Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prompts_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Friendly Tutor ===\n",
      "The Pythagorean theorem applies to right triangles. It says:\n",
      "\n",
      "- If a and b are the two short sides (the legs) and c is the longest side (the hypotenuse), then a^2 + b^2 = c^2.\n",
      "\n",
      "Simple example:\n",
      "- For a = 3 and b = 4, c^2 = 3^2 + 4^2 = 9 + 16 = 25, so c = 5.\n",
      "\n",
      "Ways to use it:\n",
      "- If you know the two legs, you can find the hypotenuse: c = sqrt(a^2 + b^2).\n",
      "- If you know one leg and the hypotenuse, you can find the other leg: b = sqrt(c^2 - a^2).\n",
      "\n",
      "Note: It only works for right triangles (one angle of 90 degrees).\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Professional Advisor ===\n",
      "Here's a structured, actionable playbook to prepare for a data science interview. It covers what to study, how to practice, and how to present yourself across common formats and company types.\n",
      "\n",
      "1) Know your target and tailor your approach\n",
      "- Roles to distinguish:\n",
      "  - Data Scientist / Applied Scientist: strong modeling, experimentation, business impact, some production thinking.\n",
      "  - ML Engineer: model deployment, scalability, pipelines, MLOps basics.\n",
      "  - Data Analyst: data storytelling, dashboards, SQL-heavy, less focus on complex modeling.\n",
      "- Company type influences emphasis:\n",
      "  - Tech giants/startups: broader ML/system design questions; rapid experimentation; scalability and cost considerations.\n",
      "  - Product-focused startups: emphasis on impact, A/B testing, feature engineering, user engagement metrics.\n",
      "  - Consulting or analytics firms: clear business framing, client communication, deliverables.\n",
      "\n",
      "2) Build a practical 8-12 week preparation plan (adjust to junior vs. senior)\n",
      "- Weeks 1-2: fundamentals reboot\n",
      "  - Statistics: probability, distributions, hypothesis testing, confidence intervals, p-values, power analysis.\n",
      "  - Linear algebra basics if needed (vectors, matrices, eigenvalues for ML intuition).\n",
      "  - Core ML concepts: bias-variance, overfitting, cross-validation, evaluation metrics (accuracy, precision/recall, ROC-AUC, log loss).\n",
      "- Weeks 3-4: coding and data manipulation\n",
      "  - Python or R proficiency: numpy, pandas, scikit-learn (fit/predict, pipelines), basic plotting.\n",
      "  - SQL mastery: joins, aggregations, window functions, CTEs; write complex queries from scratch.\n",
      "  - Practice problems: 2-3 medium-difficulty coding challenges per week.\n",
      "- Weeks 5-6: modeling and experimentation\n",
      "  - Supervised learning: linear/logistic regression, trees/ensembles, SVM, basic neural networks (conceptual).\n",
      "  - Evaluation and feature engineering: scaling, regularization, CV strategies, hyperparameter tuning.\n",
      "  - A/B testing and causal inference basics: experiment design, power calculations, lift vs. absolute metrics.\n",
      "- Weeks 7-8: systems thinking and take-home prep\n",
      "  - ML deployment basics: model monitoring, retraining triggers, data drift, feature stores (high-level).\n",
      "  - Data pipelines: ETL/ELT concepts, reproducibility, versioning (datasets, code).\n",
      "  - Start a take-home project or refine an existing one for interview-ready presentation.\n",
      "- Weeks 9-12 (optional for senior roles or dense interview schedules)\n",
      "  - System design for data products (recommendation system, fraud detector, real-time analytics).\n",
      "  - Behavioral stories and business cases; craft 3-5 solid STAR stories.\n",
      "  - Do mock interviews (technical + behavioral) and refine storytelling.\n",
      "\n",
      "3) Core technical topics to study (with a practical focus)\n",
      "- Statistics and probability\n",
      "  - Hypothesis testing, confidence intervals, p-values, power\n",
      "  - Bayesian vs frequentist intuition (high-level)\n",
      "  - A/B testing design, multiple testing corrections\n",
      "- Machine learning fundamentals\n",
      "  - Supervised learning: when to use linear models vs trees vs ensembles\n",
      "  - Regularization, bias-variance trade-off, cross-validation\n",
      "  - Feature engineering: interaction terms, encoding categorical variables, normalization\n",
      "  - Model evaluation: confusion matrix, ROC-AUC, PR-AUC, calibration\n",
      "  - Overfitting avoidance, model selection, baselines\n",
      "- Data handling\n",
      "  - Cleaning, outliers, missing data strategies\n",
      "  - Exploratory data analysis, visualization to communicate insights\n",
      "  - Data leakage pitfalls and validation discipline\n",
      "- SQL and data querying\n",
      "  - Complex joins, aggregations, window functions, common table expressions\n",
      "  - Data quality checks, deriving metrics (e.g., cohort analyses)\n",
      "- Programming and tooling\n",
      "  - Python/R fluency; pandas for data wrangling; scikit-learn for modeling\n",
      "  - Basic plotting (matplotlib/seaborn) to explain results\n",
      "  - Version control basics; reproducible notebooks/scripts\n",
      "- ML engineering and product thinking (for broader roles)\n",
      "  - End-to-end thinking: data collection, feature store, model deployment, monitoring\n",
      "  - MLOps basics: model versioning, experiment tracking, retraining triggers\n",
      "  - Privacy, fairness, and interpretability considerations\n",
      "\n",
      "4) Practice formats and concrete resources\n",
      "- Coding and data questions\n",
      "  - LeetCode (Data Science/SQL and Python-related lists), HackerRank, StrataScratch (data questions and SQL)\n",
      "  - Kaggle kernels and micro-challenges for practical datasets\n",
      "- SQL practice\n",
      "  - StrataScratch, LeetCode SQL problems, SQLZoo, Mode Analytics SQL tutorials\n",
      "- Machine learning and statistics practice\n",
      "  - \"Cracking the Data Science Interview\" (structure and questions)\n",
      "  - \"Data Scientist Interview Guide\" blogs, Glassdoor interview experiences (use for company-specific patterns)\n",
      "  - Practical notebooks: rebuild a simple dataset end-to-end (EDA, model, evaluation)\n",
      "- Take-home and projects\n",
      "  - Create or refine a project with a public dataset; document problem, data, approach, results, business impact\n",
      "  - Ensure code is clean, documented, and reproducible; include a brief write-up and a notebook with key results\n",
      "- Mock interviews\n",
      "  - Schedule 3-5 mock sessions: 1 technical, 1 behavioral, 1 system design (senior roles)\n",
      "  - Practice explaining your reasoning out loud; focus on clarity and business impact\n",
      "\n",
      "5) Behavioral and business-case prep (STAR framework)\n",
      "- Prepare 3-5 stories that cover:\n",
      "  - A challenging data problem you solved (problem, data, approach, results)\n",
      "  - A time you influenced a decision with data\n",
      "  - A failure or misstep and what you learned\n",
      "  - Cross-functional collaboration: stakeholder management, trade-offs\n",
      "  - Ambiguity: how you defined success and asked clarifying questions\n",
      "- Practice: concise (2-3 minutes) versions and longer 5-7 minute versions; tailor to the company and role\n",
      "\n",
      "6) Take-home projects: design and presentation tips\n",
      "- Clear problem statement and success criteria\n",
      "- Data description, cleaning steps, and feature engineering notes\n",
      "- Modeling approach with rationale and baselines\n",
      "- Evaluation and results, including confidence intervals or statistical significance\n",
      "- Business impact: what decisions were supported and value delivered\n",
      "- Reproducibility: provide a clean repo, instructions, and a short README\n",
      "- Be prepared to discuss trade-offs, deployment considerations, and potential biases\n",
      "\n",
      "7) System design and data product thinking (for more senior roles)\n",
      "- High-level design: data sources, ingestion, storage, processing, feature store\n",
      "- Real-time vs batch processing trade-offs\n",
      "- Model deployment lifecycle: retraining triggers, monitoring (drift, latency), rollback\n",
      "- Data privacy, governance, and security implications\n",
      "- Example prompts to practice: design a recommendations pipeline, or an anomaly detection system for streaming data\n",
      "\n",
      "8) Day-of interview logistics and etiquette\n",
      "- Bring: a concise one-page summary of 2-3 projects, a short “data science pitch” (~90 seconds), and a notebook or slides you can share\n",
      "- During the interview: think aloud; justify model choices and metrics; discuss alternatives and why you ruled them out\n",
      "- Time management: allocate roughly equal time to problem understanding, approach, and results; in coding problems, narrate your steps clearly\n",
      "- For take-home questions: ask clarifying questions upfront; outline your plan before diving in\n",
      "\n",
      "9) After the interview: reflection and next steps\n",
      "- Write a quick debrief: what went well, what to improve, any follow-up questions you’d want to answer\n",
      "- If you didn’t fully solve a problem, articulate your reasoning and partial results you achieved\n",
      "- Keep a running log of questions you struggled with to target in future practice\n",
      "- Send a brief thank-you note highlighting a couple of business insights you demonstrated\n",
      "\n",
      "10) Quick-start action for the next 48 hours\n",
      "- Pick 2-3 core topics you feel weakest on (e.g., SQL, A/B testing, ML evaluation).\n",
      "- Solve 3 practical problems in those topics (one SQL, one ML/evaluation, one coding task).\n",
      "- Pull a small take-home together: choose a public dataset, define a problem, build a simple model, and prepare a 1-page summary with business impact.\n",
      "- Draft 3 STAR stories and rehearse a 2-minute elevator pitch that explains who you are, what you care about, and a standout project.\n",
      "\n",
      "Optional: a concise, personalized plan you can adapt\n",
      "- If you’re a junior data scientist:\n",
      "  - Focus more on fundamentals, SQL, Python, and a couple of small end-to-end projects.\n",
      "  - Practice 1-2 real-world case studies with clear business impact.\n",
      "- If you’re mid-level:\n",
      "  - Add system design interview prep, ML deployment concepts, and more complex modeling problems.\n",
      "  - Build a substantial end-to-end project with a reproducible pipeline and monitoring notes.\n",
      "- If you’re senior:\n",
      "  - Emphasize leadership-driven projects, stakeholder impact, and end-to-end product thinking.\n",
      "  - Prepare multiple system-design discussions and 3-5 strong, business-impact STAR stories.\n",
      "\n",
      "Sample practice questions to get you started\n",
      "- Technical (modeling)\n",
      "  - Given a dataset with customer features and churn label, describe how you would build a churn prediction model, what features you’d engineer, how you’d validate it, and how you’d address class imbalance.\n",
      "  - How would you compare two models to decide which to deploy? What metrics would you use in production, and why?\n",
      "- Statistics and experimentation\n",
      "  - Explain how you would design an A/B test for a new feature in a mobile app. How would you determine sample size, handle multiple testing, and interpret results?\n",
      "- SQL\n",
      "  - Write a query to calculate daily active users (DAU) from user events, considering users who log in at least once per day.\n",
      "  - How would you identify cohorts based on signup date and measure retention over time?\n",
      "- Behavioral/business\n",
      "  - Tell me about a time you disagreed with a stakeholder about a modeling approach. How did you handle it, and what was the outcome?\n",
      "  - Describe a project where your analysis changed a business decision. What was the impact, and how did you quantify it?\n",
      "\n",
      "Rubric to use during practice\n",
      "- Understanding of the problem: asks clarifying questions, defines success\n",
      "- Technical correctness: appropriate methods, awareness of assumptions\n",
      "- Reasoning and justification: clear rationale, consideration of alternatives\n",
      "- Data storytelling: ability to convey insights succinctly and with impact\n",
      "- Communication: structured, concise, confident delivery\n",
      "- Business impact: translates results into measurable value or decisions\n",
      "- Reproducibility: clean code, documentation, and reproducible results\n",
      "\n",
      "If you share your target role, company types you’re aiming for, and your current strongest/weakest areas, I can tailor a specific 6-12 week plan, a set of practice problems, and a personalized list of STAR stories and mock interview questions you can use.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. Initialize LLM (Ensure this is already set up)\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-5-nano\"\n",
    ")\n",
    "\n",
    "# 2. Create the Prompt Template\n",
    "template = \"\"\"\n",
    "You are a {personality} AI assistant specialized in {domain}.\n",
    "\n",
    "User's question: {question}\n",
    "\n",
    "Provide a {tone} response.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 3. Create the Chain (Modern LCEL Pipe Syntax)\n",
    "# This replaces: chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain = prompt | llm\n",
    "\n",
    "# 4. Execution\n",
    "# We use .invoke() instead of .predict()\n",
    "\n",
    "print(\"=== Friendly Tutor ===\")\n",
    "response1 = chain.invoke({\n",
    "    \"personality\": \"friendly and encouraging\",\n",
    "    \"domain\": \"mathematics\",\n",
    "    \"question\": \"What is the Pythagorean theorem?\",\n",
    "    \"tone\": \"simple and easy to understand\"\n",
    "})\n",
    "# Note: .invoke() returns a message object, so we access .content\n",
    "print(response1.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"=== Professional Advisor ===\")\n",
    "response2 = chain.invoke({\n",
    "    \"personality\": \"professional and analytical\",\n",
    "    \"domain\": \"career development\",\n",
    "    \"question\": \"How do I prepare for a data science interview?\",\n",
    "    \"tone\": \"structured and actionable\"\n",
    "})\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chatbot_project_header",
   "metadata": {},
   "source": [
    "# 3. Building Your AI Companion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_chatbot_header",
   "metadata": {},
   "source": [
    "## 3.1 Basic Chatbot with Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_chatbot_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Alex the AI Companion:\n",
      "\n",
      "You: Hi Alex! How are you today?\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlex: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting Alex the AI Companion:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43mask_alex\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHi Alex! How are you today?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m ask_alex(\u001b[33m\"\u001b[39m\u001b[33mCan you help me with my studies?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m ask_alex(\u001b[33m\"\u001b[39m\u001b[33mWhat was the first thing I asked you?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mask_alex\u001b[39m\u001b[34m(user_input)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mask_alex\u001b[39m(user_input):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     response = \u001b[43mconversation\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlex: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5548\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5541\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5543\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5546\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5547\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5549\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5550\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5551\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5548\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5541\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5543\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5546\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5547\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5549\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5550\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5551\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3143\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3141\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3142\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3143\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3145\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5548\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5541\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5543\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5546\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5547\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5549\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5550\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5551\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4871\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4856\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this `Runnable` synchronously.\u001b[39;00m\n\u001b[32m   4857\u001b[39m \n\u001b[32m   4858\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4868\u001b[39m \n\u001b[32m   4869\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4872\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4873\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4875\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4876\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4877\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4878\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2058\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2054\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2056\u001b[39m         output = cast(\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2058\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2060\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2066\u001b[39m         )\n\u001b[32m   2067\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2068\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:433\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    432\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4739\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4735\u001b[39m         msg = (\n\u001b[32m   4736\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit reached when invoking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4737\u001b[39m         )\n\u001b[32m   4738\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRecursionError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m4739\u001b[39m     output = \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4740\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4742\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4743\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4744\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrecursion_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecursion_limit\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4745\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4746\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4747\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5548\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5541\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5543\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5546\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5547\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5549\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5550\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5551\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3143\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3141\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3142\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3143\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3145\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1380\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1378\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1379\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1383\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1384\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1385\u001b[39m ):\n\u001b[32m   1386\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1375\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1369\u001b[39m             response,\n\u001b[32m   1370\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1371\u001b[39m             metadata=generation_info,\n\u001b[32m   1372\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1373\u001b[39m         )\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m         response = raw_response.parse()\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/langchain_workshop/.venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# 1. Define chatbot personality\n",
    "# We use a 'System' message to define Alex's persona\n",
    "chatbot_personality = \"\"\"\n",
    "You are Alex, a friendly and knowledgeable AI companion. You are:\n",
    "- Patient and understanding\n",
    "- Enthusiastic about learning and helping\n",
    "- Conversational and warm\n",
    "- Clear and concise in explanations\n",
    "Always respond in a friendly, natural way. Keep responses focused and helpful.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create the modern Chat Prompt Template\n",
    "# This replaces the old 'PromptTemplate' and handles memory automatically\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", chatbot_personality),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 3. Initialize Azure LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-35-turbo\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 4. Memory Management (The Window Memory Logic)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        # ChatMessageHistory stores all messages for the session\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    \n",
    "    # Implementing 'Window' logic (k=5)\n",
    "    # This keeps the context window clean for high-performance inference\n",
    "    if len(store[session_id].messages) > 10: # k=5 interactions * 2 messages each\n",
    "        store[session_id].messages = store[session_id].messages[-10:]\n",
    "        \n",
    "    return store[session_id]\n",
    "\n",
    "# 5. Build the modern chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Wrap with history logic\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# 6. Test the Chatbot\n",
    "config = {\"configurable\": {\"session_id\": \"alex_test_1\"}}\n",
    "\n",
    "def ask_alex(user_input):\n",
    "    print(f\"You: {user_input}\")\n",
    "    response = conversation.invoke({\"input\": user_input}, config)\n",
    "    print(f\"Alex: {response.content}\\n\")\n",
    "\n",
    "print(\"Testing Alex the AI Companion:\\n\")\n",
    "ask_alex(\"Hi Alex! How are you today?\")\n",
    "ask_alex(\"Can you help me with my studies?\")\n",
    "ask_alex(\"What was the first thing I asked you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chatbot_class_header",
   "metadata": {},
   "source": [
    "## 3.2 Reusable Chatbot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chatbot_class_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1: Study Buddy ===\n",
      "StudyBot: Sorry, I encountered an error: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n",
      "\n",
      "=== Example 2: Wellness Coach ===\n",
      "WellnessBot: Sorry, I encountered an error: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "class AICompanion:\n",
    "    def __init__(self, name, personality, domain=None, memory_window=5):\n",
    "        self.name = name\n",
    "        self.personality = personality\n",
    "        self.domain = domain\n",
    "        self.memory_window = memory_window\n",
    "        \n",
    "        # 1. Initialize modern Azure LLM\n",
    "        # Ensure 'deployment_name' matches your Azure portal\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-35-turbo\"),\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # 2. Storage for session history\n",
    "        self.store = {}\n",
    "        self._setup_conversation()\n",
    "\n",
    "    def _get_session_history(self, session_id: str):\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        \n",
    "        # Implement Window Memory (k=memory_window)\n",
    "        # 1 interaction = 2 messages (Human + AI)\n",
    "        max_messages = self.memory_window * 2\n",
    "        if len(self.store[session_id].messages) > max_messages:\n",
    "            self.store[session_id].messages = self.store[session_id].messages[-max_messages:]\n",
    "        return self.store[session_id]\n",
    "\n",
    "    def _setup_conversation(self):\n",
    "        # 3. Build System Prompt\n",
    "        system_msg = f\"You are {self.name}. {self.personality}\"\n",
    "        if self.domain:\n",
    "            system_msg += f\"\\n\\nYou specialize in {self.domain}.\"\n",
    "        system_msg += \"\\n\\nProvide helpful, clear, and friendly responses.\"\n",
    "\n",
    "        # 4. Modern ChatPrompt with Placeholder for history\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_msg),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "\n",
    "        # 5. Create Chain (LCEL) and wrap with history\n",
    "        chain = prompt | self.llm\n",
    "        self.conversation = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            self._get_session_history,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "    def chat(self, user_input, session_id=\"default\"):\n",
    "        try:\n",
    "            # Use .invoke() instead of .predict()\n",
    "            config = {\"configurable\": {\"session_id\": session_id}}\n",
    "            response = self.conversation.invoke({\"input\": user_input}, config)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "    def reset_memory(self, session_id=\"default\"):\n",
    "        if session_id in self.store:\n",
    "            self.store[session_id].clear()\n",
    "            print(f\"{self.name}'s memory for {session_id} has been cleared!\")\n",
    "\n",
    "# --- Examples ---\n",
    "print(\"=== Example 1: Study Buddy ===\")\n",
    "study_buddy = AICompanion(\n",
    "    name=\"StudyBot\",\n",
    "    personality=\"You are patient and encouraging.\",\n",
    "    domain=\"mathematics and science\"\n",
    ")\n",
    "print(f\"StudyBot: {study_buddy.chat('Explain Newton\\'s first law')}\\n\")\n",
    "\n",
    "print(\"=== Example 2: Wellness Coach ===\")\n",
    "wellness_coach = AICompanion(\n",
    "    name=\"WellnessBot\",\n",
    "    personality=\"You are supportive and motivating.\",\n",
    "    domain=\"health and wellness\"\n",
    ")\n",
    "print(f\"WellnessBot: {wellness_coach.chat('Give me tips for better sleep')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive_chatbot_header",
   "metadata": {},
   "source": [
    "## 3.3 Interactive Console Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "interactive_chatbot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interactive_chatbot():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session in the console.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Interactive AI Companion\")\n",
    "    print(\"Commands: 'quit' to exit, 'reset' to clear memory\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Create chatbot\n",
    "    bot = AICompanion(\n",
    "        name=\"Companion\",\n",
    "        personality=\"You are friendly, helpful, and conversational.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Companion: Hi! I'm your AI companion. How can I help you today?\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        # Check for commands\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nCompanion: Goodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'reset':\n",
    "            bot.reset_memory()\n",
    "            continue\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Get and display response\n",
    "        response = bot.chat(user_input)\n",
    "        print(f\"\\nCompanion: {response}\\n\")\n",
    "\n",
    "# Run the interactive chatbot\n",
    "# Uncomment the line below to start chatting!\n",
    "# run_interactive_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agents_header",
   "metadata": {},
   "source": [
    "# 4. Advanced: Agents with Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agents_intro",
   "metadata": {},
   "source": [
    "## 4.1 Understanding Agents\n",
    "\n",
    "**Agents** are LLMs that can:\n",
    "- Use external tools\n",
    "- Make decisions about which tool to use\n",
    "- Chain multiple tool calls together\n",
    "\n",
    "Think of agents as \"AI that can DO things\" beyond just chatting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_tools_header",
   "metadata": {},
   "source": [
    "## 4.2 Creating Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "custom_tools_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartChatbot:\n",
    "    def __init__(self, name, tools_list):\n",
    "        self.name = name\n",
    "        self.tools = tools_list\n",
    "        \n",
    "        # 1. Initialize modern LLM\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            azure_deployment=\"gpt-35-turbo\", \n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # 2. Storage for session history (Replacing Legacy Memory)\n",
    "        self.store = {}\n",
    "\n",
    "        # 3. Define the new Agent (built on LangGraph)\n",
    "        self.agent = create_agent(\n",
    "            model=self.llm,\n",
    "            tools=self.tools,\n",
    "            system_prompt=f\"You are {self.name}, an advanced robotics assistant.\"\n",
    "        )\n",
    "\n",
    "    def _get_history(self, session_id: str):\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "\n",
    "    def chat(self, user_input, session_id=\"default\"):\n",
    "        try:\n",
    "            # v1.0 uses .invoke() with a list of messages\n",
    "            runnable = RunnableWithMessageHistory(\n",
    "                self.agent,\n",
    "                self._get_history,\n",
    "                input_messages_key=\"messages\", # Standard for v1.0 agents\n",
    "                history_messages_key=\"history\",\n",
    "            )\n",
    "            config = {\"configurable\": {\"session_id\": session_id}}\n",
    "            result = runnable.invoke({\"messages\": [(\"user\", user_input)]}, config)\n",
    "            \n",
    "            # Returns the content of the final AI message\n",
    "            return result['messages'][-1].content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent_chatbot_header",
   "metadata": {},
   "source": [
    "## 4.3 Chatbot with Tools and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "agent_chatbot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchainhub as hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "# New v1.0 Agent abstraction\n",
    "from langchain.agents import create_agent\n",
    "# Persistence replaces legacy ConversationBufferMemory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "class SmartChatbot:\n",
    "    \"\"\"\n",
    "    v1.0 Chatbot using create_agent and LangGraph persistence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, tools_list):\n",
    "        self.name = name\n",
    "        self.tools = tools_list\n",
    "        \n",
    "        # 1. Initialize modern Azure LLM\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            azure_deployment=\"gpt-35-turbo\", # Use actual deployment name\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # 2. Setup checkpointer for memory\n",
    "        # MemorySaver acts as the modern short-term 'thread' memory\n",
    "        self.checkpointer = MemorySaver()\n",
    "        \n",
    "        # 3. Create the v1.0 Agent\n",
    "        # create_agent replaces AgentExecutor + create_react_agent\n",
    "        self.agent = create_agent(\n",
    "            model=self.llm,\n",
    "            tools=self.tools,\n",
    "            system_prompt=f\"You are {self.name}, an advanced robotics assistant.\",\n",
    "            checkpointer=self.checkpointer\n",
    "        )\n",
    "\n",
    "    def chat(self, user_input, thread_id=\"default_thread\"):\n",
    "        \"\"\"\n",
    "        Chat using the new .invoke() interface.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # v1.0 standard input is a list of messages\n",
    "            # 'thread_id' determines which conversation history to retrieve\n",
    "            config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "            result = self.agent.invoke(\n",
    "                {\"messages\": [(\"user\", user_input)]}, \n",
    "                config\n",
    "            )\n",
    "            # The last message in the list is the AI's final response\n",
    "            return result['messages'][-1].content\n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "    def list_tools(self):\n",
    "        return [tool.name for tool in self.tools]\n",
    "\n",
    "# Usage\n",
    "smart_bot = SmartChatbot(name=\"SmartBot\", tools_list=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production_header",
   "metadata": {},
   "source": [
    "# 5. Production-Ready Chatbot Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "production_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import langchainhub as hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "# Modern v1.0 imports\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "class ProductionChatbot:\n",
    "    \"\"\"\n",
    "    A v1.0-ready chatbot framework for Robotics/Physical AI.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.name = config.get('name', 'Assistant')\n",
    "        self.personality = config.get('personality', 'You are a helpful AI assistant.')\n",
    "        self.domain = config.get('domain')\n",
    "        self.memory_window = config.get('memory_window', 5)\n",
    "        self.temperature = config.get('temperature', 0.7)\n",
    "        self.tools = config.get('tools', [])\n",
    "        \n",
    "        self.conversation_log = []\n",
    "        \n",
    "        # 1. Initialize modern Azure LLM\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            azure_deployment=\"gpt-35-turbo\", # Match your Azure Portal\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        \n",
    "        # 2. Setup Persistence (Replaces ConversationBufferWindowMemory)\n",
    "        self.checkpointer = MemorySaver()\n",
    "        \n",
    "        # 3. Setup the Agent (Handles both chain and tool modes)\n",
    "        system_prompt = f\"You are {self.name}. {self.personality}\"\n",
    "        if self.domain:\n",
    "            system_prompt += f\" You specialize in {self.domain}.\"\n",
    "            \n",
    "        self.agent = create_agent(\n",
    "            model=self.llm,\n",
    "            tools=self.tools,\n",
    "            system_prompt=system_prompt,\n",
    "            checkpointer=self.checkpointer\n",
    "        )\n",
    "    \n",
    "    def chat(self, user_input, thread_id=\"robot_session_1\"):\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        try:\n",
    "            # v1.0 uses .invoke() with a message list\n",
    "            result = self.agent.invoke(\n",
    "                {\"messages\": [(\"user\", user_input)]}, \n",
    "                config\n",
    "            )\n",
    "            response = result['messages'][-1].content\n",
    "            \n",
    "            self._log_interaction(timestamp, user_input, response, True)\n",
    "            return {'response': response, 'timestamp': timestamp, 'success': True}\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            self._log_interaction(timestamp, user_input, error_msg, False, str(e))\n",
    "            return {\n",
    "                'response': \"Sorry, I encountered an error processing your request.\",\n",
    "                'timestamp': timestamp,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def _log_interaction(self, ts, user, assistant, success, error=None):\n",
    "        log_entry = {\n",
    "            'timestamp': ts, 'user': user, 'assistant': assistant, 'success': success\n",
    "        }\n",
    "        if error: log_entry['error'] = error\n",
    "        self.conversation_log.append(log_entry)\n",
    "\n",
    "    def reset(self, thread_id=\"robot_session_1\"):\n",
    "        \"\"\"Resets specific thread memory\"\"\"\n",
    "        # Note: In MemorySaver, you typically just start a new thread_id\n",
    "        self.conversation_log.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'action': f'reset_request_for_{thread_id}'\n",
    "        })\n",
    "\n",
    "    def get_stats(self):\n",
    "        total = len([log for log in self.conversation_log if 'user' in log])\n",
    "        successful = len([log for log in self.conversation_log if log.get('success')])\n",
    "        return {\n",
    "            'total_exchanges': total,\n",
    "            'successful': successful,\n",
    "            'success_rate': f\"{(successful/total*100):.1f}%\" if total > 0 else \"0%\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices_header",
   "metadata": {},
   "source": [
    "# 6. Best Practices & Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae8963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "best_practices_content",
   "metadata": {},
   "source": [
    "##  Key Best Practices\n",
    "\n",
    "### 1. **Memory Management**\n",
    "```python\n",
    "#  Good: Use window memory to control costs\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "\n",
    "#  Avoid: Unlimited buffer for long conversations\n",
    "# memory = ConversationBufferMemory()  # Can get expensive!\n",
    "```\n",
    "\n",
    "### 2. **Temperature Selection**\n",
    "- **0.0-0.3**: Factual tasks, consistent outputs\n",
    "- **0.7**: Balanced (recommended for most cases)\n",
    "- **0.9-1.0**: Creative writing, varied responses\n",
    "\n",
    "### 3. **Error Handling**\n",
    "```python\n",
    "try:\n",
    "    response = chatbot.chat(user_input)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    response = \"Sorry, something went wrong.\"\n",
    "```\n",
    "\n",
    "### 4. **Prompt Engineering**\n",
    "- Be specific and clear\n",
    "- Provide context and examples\n",
    "- Define personality and constraints\n",
    "- Keep system prompts focused\n",
    "\n",
    "### 5. **API Cost Optimization**\n",
    "- Limit conversation history\n",
    "- Use appropriate temperature\n",
    "- Cache common responses\n",
    "- Monitor token usage\n",
    "\n",
    "### 6. **Security**\n",
    "- Never commit API keys\n",
    "- Use environment variables\n",
    "- Validate user inputs\n",
    "- Implement rate limiting\n",
    "\n",
    "### 7. **Testing**\n",
    "```python\n",
    "test_cases = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What can you help me with?\",\n",
    "    \"Tell me about yourself\"\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    response = chatbot.chat(test)\n",
    "    print(f\"Input: {test}\")\n",
    "    print(f\"Output: {response}\\n\")\n",
    "```\n",
    "\n",
    "### 8. **Code Organization**\n",
    "```\n",
    "project/\n",
    "├── chatbot.py          # Main chatbot class\n",
    "├── tools.py            # Custom tool definitions\n",
    "├── config.py           # Configuration\n",
    "├── .env                # API keys (gitignored!)\n",
    "```\n",
    "\n",
    "### 9. **Common Pitfalls to Avoid**\n",
    "- Storing sensitive data in prompts\n",
    "- Not handling API failures\n",
    "- Unlimited conversation history\n",
    "- Overly complex system prompts\n",
    "- Not testing edge cases\n",
    "\n",
    "### 10. **Deployment Checklist**\n",
    "- [ ] Environment variables configured\n",
    "- [ ] Error handling implemented\n",
    "- [ ] Memory limits set\n",
    "- [ ] Logging enabled\n",
    "- [ ] Rate limiting considered\n",
    "- [ ] API keys secured\n",
    "- [ ] Testing completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheatsheet_header",
   "metadata": {},
   "source": [
    "# 7. Quick Reference Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheatsheet_content",
   "metadata": {},
   "source": [
    "## Essential Code Snippets\n",
    "\n",
    "### Initialize LLM\n",
    "```python\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0.7\n",
    ")\n",
    "```\n",
    "\n",
    "### Create Memory\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "```\n",
    "\n",
    "### Simple Conversation\n",
    "```python\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "response = conversation.predict(input=\"Hello!\")\n",
    "```\n",
    "\n",
    "### Custom Prompt\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"You are {role}. {input}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"role\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "```\n",
    "\n",
    "### Create Tool\n",
    "```python\n",
    "from langchain.agents import Tool\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"ToolName\",\n",
    "    func=your_function,\n",
    "    description=\"What the tool does\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Error Handling\n",
    "```python\n",
    "try:\n",
    "    response = chatbot.chat(user_input)\n",
    "except Exception as e:\n",
    "    response = f\"Error: {e}\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccds-tech-for-good-hackathon-2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
