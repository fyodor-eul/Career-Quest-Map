{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRTXzXxM9uJk"
      },
      "source": [
        "## CCDS Tech for Good 2026 Hackathon\n",
        "### Beautiful Soup Workshop Contents\n",
        "#### Created by James, Denzyl, Ronav\n",
        "\n",
        "**Workshop Goals:**\n",
        "- Understand HTML basics\n",
        "- Identify structure of HTML\n",
        "- Locate components to get the info you want\n",
        "- Scrape the website's data\n",
        "- Parse the data to get what you need\n",
        "\n",
        "**Workshop Content**\n",
        "1. Ethics and legality of web scraping\n",
        "2. Basics of HTML\n",
        "3. Inspecting your website\n",
        "4. Scraping HTML from a website with requests\n",
        "5. Parsing HTML with BeautifulSoup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNvY9alnSYev"
      },
      "source": [
        "## Section 1: Ethics and legality of web scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk8FpxCvbTZ8"
      },
      "source": [
        "Web scraping is the process of gathering information from the internet. Even copying and pasting the lyrics of your favorite song can be considered a form of web scraping! However, the term “web scraping” usually refers to a process that involves automation. While some websites don’t like it when automatic scrapers gather their data, which can lead to legal issues, others don’t mind it.\n",
        "\n",
        "If you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Still, it’s a good idea to do some research on your own to make sure you’re not violating any Terms of Service before you start a large-scale web scraping project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mawj3I7ia8jj"
      },
      "source": [
        "\"Using Beautiful Soup is legal because you only use it for parsing documents. Web scraping in general is also legal if you respect a website’s terms of service and copyright laws.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8zLHP5qWYng"
      },
      "source": [
        "## Section 2: Basics of HTML\n",
        "\n",
        "Hypertext Markup Language (HTML) is the standard markup language for Web pages.\n",
        "\n",
        "A HTML element is defined by a starting tag, content and an ending tag.\n",
        "\n",
        "```<tagname> Content goes here... </tagname>```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63fa18d0"
      },
      "source": [
        "### HTML Attributes\n",
        "Beyond just tags and content, HTML elements can have **attributes**. These provide additional information about the element. Attributes are usually key-value pairs inside the opening tag.\n",
        "\n",
        "For example:\n",
        "\n",
        "```html\n",
        "<a href=\"https://www.example.com\" class=\"external-link\">Click here</a>\n",
        "```\n",
        "Here, `href` and `class` are attributes of the `<a>` (anchor) tag. `BeautifulSoup` uses these attributes (especially `id` and `class`) to find specific elements on a page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "764e7a2e"
      },
      "source": [
        "### Nesting of HTML Elements (Parent-Child Relationships)\n",
        "HTML elements are often nested inside one another, creating a hierarchical structure, similar to a family tree. An element inside another is called a 'child', and the enclosing element is its 'parent'.\n",
        "\n",
        "```html\n",
        "<div> <!-- Parent -->\n",
        "  <p> <!-- Child of div -->\n",
        "    <span> <!-- Child of p -->\n",
        "      Hello World\n",
        "    </span>\n",
        "  </p>\n",
        "</div>\n",
        "```\n",
        "\n",
        "Understanding this nesting is useful because `BeautifulSoup` allows us to navigate up and down this tree (e.g., finding the 'parent' of an element, or children within a parent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb892dda"
      },
      "source": [
        "### Common HTML Tags You'll Encounter\n",
        "While there are many HTML tags, some are more common in web scraping:\n",
        "\n",
        "*   `<p>`: Paragraph of text.\n",
        "*   `<a>`: Link (anchor). The actual URL is usually in the `href` attribute.\n",
        "*   `<div>`: Division or section. Often used to group other elements and define layout.\n",
        "*   `<span>`: An inline container for small pieces of content.\n",
        "*   `<h1>` to `<h6>`: Headings (from largest to smallest).\n",
        "*   `<ul>`, `<ol>`, `<li>`: Unordered lists, ordered lists, and list items.\n",
        "*   `<img>`: Image. The source URL is in the `src` attribute.\n",
        "\n",
        "Knowing these helps you quickly identify what kind of content you're looking at when inspecting a web page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82940e23"
      },
      "source": [
        "### `id` and `class` Attributes\n",
        "While any attribute can be used, `id` and `class` are exceptionally useful for pinpointing specific elements:\n",
        "\n",
        "*   **`id`**: Stands for \"identifier\". An `id` attribute should be **unique** within an HTML document. This makes it perfect for finding a single, specific element quickly.\n",
        "    *   Example: `<div id=\"main-content\">...</div>`\n",
        "\n",
        "*   **`class`**: Used to classify elements. Multiple elements can share the same `class` attribute. This is great for finding groups of similar elements (e.g., all job cards, all navigation links).\n",
        "    *   Example: `html<p class=\"job-title\">...</p>`\n",
        "    *   In BeautifulSoup: `soup.find_all(\"p\", class_=\"job-title\")` (Note the underscore after `class` in Python to avoid conflict with the `class` keyword).\n",
        "\n",
        "When inspecting a web page, always look for these attributes first, as they often provide the easiest way to target the data you want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1988bd0"
      },
      "source": [
        "### Reinforcing 'Inspect Element'\n",
        "Constantly remind participants that the most powerful tool for web scraping is their browser's **Developer Tools** (usually accessed by `F12` or `Right Click + Inspect`). They need to practice:\n",
        "\n",
        "1.  **Selecting an element:** To see its HTML code, tags, and attributes.\n",
        "2.  **Navigating the HTML tree:** To understand parent-child relationships and identify unique selectors (`id`, `class`).\n",
        "\n",
        "This hands-on inspection is how they will figure out what to tell `BeautifulSoup` to find!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4luWAC3KVXXy"
      },
      "source": [
        "## Section 3: Inspecting your target website\n",
        "Now we are abit more familiar with HTML, let's see it on a website.\n",
        "\n",
        "We will use this website for this workshop: https://realpython.github.io/fake-jobs/\n",
        "\n",
        "Open Developer Tools by ```F12``` or ```Right Click + Inspect```\n",
        "\n",
        "Specifically find an element by highlighting the text and press ```Right Click + Inspect```\n",
        "\n",
        "Try to identify some of the structure earlier by using the Developer Tools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fors-vyMEex2"
      },
      "source": [
        "## Section 4: Scraping HTML from a website\n",
        "\n",
        "Why do we want to scrape from website? \n",
        "\n",
        "In this case, let's say we are looking to apply to only Python jobs on a job search website, but there are no features to search the website and there are other jobs included like Java or other languages. We can use BeautifulSoup to find all job titles with Python, then we gather the company name, location and application link for our convenience.\n",
        "\n",
        "Run the following code cell and look at the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'base (Python 3.11.9)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
          ]
        }
      ],
      "source": [
        "# module imports\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we send a HTTP GET request to the URL and get the HTML source code and save it into the page variable.\n",
        "\n",
        "What is a HTTP GET request?\n",
        "HTTP or Hypertext Transfer Protocol is the interface in which how web browsers request and receive data. There are other HTTP methods such as GET, POST, PUT, DELETE etc.\n",
        "We use GET to request data from the web server.\n",
        "\n",
        "Run the cell to see the pages source code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "URL = \"https://realpython.github.io/fake-jobs/\"\n",
        "page = requests.get(URL)\n",
        "\n",
        "print(page.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChNA6-myVAGQ"
      },
      "source": [
        "The website given has stored all the job cards under the identifier ```<div id=\"ResultsContainer\">```.\n",
        "\n",
        "Here we use soup to find the tags with the ```id=\"ResultsContainer\"``` and store it in the ```results``` variable.\n",
        "\n",
        "Run the cell the ```results``` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "results = soup.find(id=\"ResultsContainer\")\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCMFX94sF_gL"
      },
      "source": [
        "## Section 5: Parsing the HTML\n",
        "In this section we will go through how to get the information you need from the html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKCdVnv4MUuN"
      },
      "source": [
        "We search through the whole ```results``` html and find all the ```<h2>``` tags with \"python\" in the content and add it to the ```python_jobs``` list.\n",
        "\n",
        "Run the next cell to see whats inside the ```python_jobs``` variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V2SLcd2MIAk",
        "outputId": "aecd6bae-0aab-4e23-d926-043afd27a62c"
      },
      "outputs": [],
      "source": [
        "python_jobs = results.find_all(\n",
        "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
        ")\n",
        "\n",
        "print(python_jobs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7M0fEOOPRRl"
      },
      "source": [
        "Now for each ```<h2>``` tagged element in the ```python_jobs``` list, we want to get the parent 3 levels up by calling ```.parent``` 3 times. We do this because all the information we need such as title, company and location will be available in the content of this parent element.\n",
        "\n",
        "```python_job_cards``` holds all the parent for each h2 element we scraped earlier.\n",
        "\n",
        "\n",
        "Run the next cell to see whats inside the ```python_jobs_cards``` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzl7QSlOPLf3"
      },
      "outputs": [],
      "source": [
        "python_job_cards = [\n",
        "    h2_element.parent.parent.parent for h2_element in python_jobs\n",
        "]\n",
        "print(python_job_cards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noj1CBx8H5GR"
      },
      "source": [
        "The following is a snippet of html from the website. The ```<div>``` element with the card-content class contains all the information you want. It’s 3 levels up from the ```<h2>``` title element that you found using your filter. i.e Parent of ```<h2 class=\"title is-5\">``` is ```<div class=\"media-content\">```, the grandparent is ```<div class=\"media\">```, the greatgrandparent is ```<div class=\"card-content\">```.\n",
        "```\n",
        "<div class=\"card\">\n",
        "  <div class=\"card-content\">\n",
        "    <div class=\"media\">\n",
        "      <div class=\"media-left\">\n",
        "        <figure class=\"image is-48x48\">\n",
        "          <img\n",
        "            src=\"https://files.realpython.com/media/real-python-logo-thumbnail.7f0db70c2ed2.jpg\"\n",
        "            alt=\"Real Python Logo\"\n",
        "          />\n",
        "        </figure>\n",
        "      </div>\n",
        "      <div class=\"media-content\">\n",
        "        <h2 class=\"title is-5\">Senior Python Developer</h2>\n",
        "\n",
        "        <h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
        "      </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"content\">\n",
        "      <p class=\"location\">Stewartbury, AA</p>\n",
        "      <p class=\"is-small has-text-grey\">\n",
        "        <time datetime=\"2021-04-08\">2021-04-08</time>\n",
        "      </p>\n",
        "    </div>\n",
        "    <footer class=\"card-footer\">\n",
        "      <a\n",
        "        href=\"https://www.realpython.com\"\n",
        "        target=\"_blank\"\n",
        "        class=\"card-footer-item\"\n",
        "        >Learn</a\n",
        "      >\n",
        "      <a\n",
        "        href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\"\n",
        "        target=\"_blank\"\n",
        "        class=\"card-footer-item\"\n",
        "        >Apply</a\n",
        "      >\n",
        "    </footer>\n",
        "  </div>\n",
        "</div>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUqcJLqlRuEp"
      },
      "source": [
        "Putting it all together, we have a script that scraped the website and finds jobs with python in the title along with company and location as well as the application link.\n",
        "\n",
        "Run the next cell to see the result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OdFLDt-Fw5y"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = \"https://realpython.github.io/fake-jobs/\"\n",
        "page = requests.get(URL)\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "results = soup.find(id=\"ResultsContainer\")\n",
        "\n",
        "\n",
        "python_jobs = results.find_all(\n",
        "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
        ")\n",
        "\n",
        "python_job_cards = [\n",
        "    h2_element.parent.parent.parent for h2_element in python_jobs\n",
        "]\n",
        "\n",
        "for job_card in python_job_cards:\n",
        "    title_element = job_card.find(\"h2\", class_=\"title\")\n",
        "    company_element = job_card.find(\"h3\", class_=\"company\")\n",
        "    location_element = job_card.find(\"p\", class_=\"location\")\n",
        "    print(\"Job title:\", title_element.text.strip())\n",
        "    print(\"Company Name:\",company_element.text.strip())\n",
        "    print(\"Location:\",location_element.text.strip())\n",
        "    link_url = job_card.find_all(\"a\")[1][\"href\"]\n",
        "    print(f\"Apply here: {link_url}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This workshop was based on the following references:\n",
        "* https://realpython.com/beautiful-soup-web-scraper-python/\n",
        "* https://www.w3schools.com/html/\n",
        "<br>\n",
        "\n",
        "Other resources for self learning:\n",
        "* [FreeCodeCamp's BeautifulSoup Crash Course](https://www.youtube.com/watch?v=XVv6mJpFOb0)\n",
        "* [Tinkernut's Beginner Guide to Web Scraping with Python](https://www.youtube.com/watch?v=QhD015WUMxE&pp=ygUTd2ViIHNjcmFwaW5nIHB5dGhvbg%3D%3D)\n",
        "* [Corey Schafer's Web Scraping with Beautiful Soup and Requests](https://youtu.be/ng2o98k983k)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
